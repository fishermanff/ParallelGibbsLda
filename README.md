## ParallelGibbsLda  
LDA（Latent Dirichlet Allocation）由Blei于2003年提出，是机器学习中一种重要的隐变量挖掘模型（topic model），目前被广泛应用于文本挖掘、社交分析、计算广告等领域；  
LDA的数学推导十分复杂，但基于Gibbs Sampling的工程实现却相对简单，作者在实习期间首次接触topic model，深感其中数学与工程之美；  

***
训练语料来源于某站点新闻资讯频道，由于未获授权，本项目仅抽取少量的2000篇新闻文本做测试；  
resource目录下的语料已预先进行分词、去停用词；另外，将低频噪声词过滤掉会显著提升模型perplexity；  
因此，resource目录下同时包含一份训练语料中的高频词
***

项目由两部分组成，第一部分实现传统单线程Gibbs采样，第二部分实现AD-LDA并行Gibbs采样

### 单线程

    Hyperparameters： K=30，alpha=2.0，beta=0.5，iter=200； 
    
    training process cost 39s
	after training, the corpus perplexity is 703.0678
	topic-word martrix：  
	topic 0 >>> 旅游:0.0294 中国:0.0197 酒店:0.0170 文化:0.0165 城市:0.0163 景区:0.0149 公园:0.0138 活动:0.0125 大理:0.0112 体验:0.0108  
	topic 1 >>> 风格:0.0005 机动车:0.0004 前台:0.0003 一群:0.0003 女孩:0.0003 条件:0.0003 能力:0.0003 开放:0.0003 有史以来:0.0002 贵人:0.0002  
	topic 2 >>> 魅力:0.0085 不错:0.0038 特别:0.0006 结婚:0.0005 第一:0.0005 实验:0.0004 印度:0.0004 一件:0.0004 宝宝:0.0004 网友:0.0004  
	topic 3 >>> 工作:0.0151 公司:0.0125 两个:0.0104 人员:0.0104 发展:0.0102 站点:0.0095 交易:0.0090 办理:0.0084 服务:0.0084 尝试:0.0079  
	topic 4 >>> 美国:0.0672 展开:0.0006 诊断:0.0005 一杯:0.0005 视频:0.0005 保鲜:0.0004 天津市:0.0004 公共:0.0004 国内:0.0004 城市:0.0004  
	topic 5 >>> 建议:0.0280 女性:0.0077 公交:0.0011 体质:0.0005 回家:0.0005 一道:0.0005 衣物:0.0004 城乡:0.0004 判断:0.0003 话题:0.0003  
	topic 6 >>> 武汉:0.0255 研发:0.0223 呼吸:0.0217 电话:0.0196 地址:0.0131 最为:0.0056 调整:0.0033 高跟鞋:0.0032 烤肉:0.0020 统筹:0.0019  
	topic 7 >>> 简介:0.0142 免费:0.0006 第一步:0.0005 瞬间:0.0005 微信:0.0005 不知:0.0004 蜂蜜:0.0004 即可:0.0004 翻滚:0.0003 第一天:0.0003  
	topic 8 >>> 做法:0.0187 适量:0.0163 分钟:0.0152 鸡蛋:0.0144 即可:0.0120 食物:0.0115 营养:0.0111 倒入:0.0100 食用:0.0100 习惯:0.0095  
	topic 9 >>> 恐怖:0.0108 适合:0.0052 也许:0.0033 美女:0.0015 增加:0.0006 零食:0.0005 超级:0.0005 娱乐:0.0005 一份:0.0005 昨日:0.0004  
	topic 10 >>> 东方:0.0105 客厅:0.0079 影视:0.0078 过瘾:0.0066 没什么:0.0061 村民:0.0061 发酵:0.0057 两旁:0.0005 香蕉:0.0005 周岁:0.0004  
	topic 11 >>> 心灵:0.0007 满满:0.0007 一点:0.0006 欧式:0.0004 增加:0.0004 武汉:0.0004 钥匙:0.0003 特价:0.0003 不知:0.0003 房子:0.0003  
	topic 12 >>> 健康:0.0009 主流:0.0004 服装:0.0004 规模:0.0004 香甜:0.0003 气温:0.0003 负责:0.0003 房子:0.0003 控制:0.0003 赶紧:0.0003  
	topic 13 >>> 网友:0.0006 贵人:0.0005 资格:0.0005 音乐:0.0005 告诉:0.0005 辜负:0.0004 机票:0.0004 茄子:0.0004 大理:0.0004 姐妹:0.0003  
	topic 14 >>> 肯定:0.0004 一份:0.0004 公交:0.0004 无处:0.0003 时髦:0.0003 杀手:0.0003 蜂蜜:0.0003 训练:0.0003 工程:0.0003 各家:0.0002  
	topic 15 >>> 海鲜:0.0153 学会:0.0152 贷款:0.0117 饭店:0.0112 客人:0.0094 家常菜:0.0092 料理:0.0033 野外:0.0005 质量:0.0005 统筹:0.0004  
	topic 16 >>> 刻画:0.0006 欧式:0.0003 米其林:0.0003 零食:0.0003 近日:0.0003 玉米:0.0003 汽车:0.0003 一份:0.0003 下午:0.0003 钥匙:0.0002  
	topic 17 >>> 米其林:0.0006 明天:0.0005 顾客:0.0005 厨房:0.0004 网友:0.0004 丛生:0.0003 印记:0.0003 开心:0.0003 美女:0.0003 公布:0.0003  
	topic 18 >>> 利润:0.0006 厨房:0.0006 超级:0.0005 石头:0.0004 校服:0.0004 形式:0.0004 朋友:0.0004 鸡皮:0.0003 杀手:0.0003 不知:0.0003  
	topic 19 >>> 电影:0.0801 国内:0.0266 作品:0.0195 故事:0.0178 导演:0.0167 演员:0.0160 拍摄:0.0148 角色:0.0144 观众:0.0140 厦门:0.0139  
	topic 20 >>> null:0.0846 蚕丝:0.0113 有史以来:0.0110 贵人:0.0107 结膜:0.0103 吃透:0.0102 牵挂:0.0098 坠入:0.0094 丛生:0.0092 饭庄:0.0089  
	topic 21 >>> 美食:0.0260 地址:0.0257 味道:0.0217 推荐:0.0194 好吃:0.0165 机动车:0.0153 餐厅:0.0143 电话:0.0134 人均:0.0129 牛肉:0.0110  
	topic 22 >>> 几个:0.0006 辜负:0.0004 上午:0.0004 女孩:0.0004 完美:0.0004 关注:0.0004 姐妹:0.0003 车祸:0.0003 发挥:0.0003 来到:0.0003  
	topic 23 >>> 统筹:0.0058 出发:0.0058 感受:0.0054 携手:0.0049 站点:0.0046 时光:0.0042 爱好者:0.0005 国际:0.0005 坠入:0.0004 邀请:0.0004  
	topic 24 >>> 餐桌:0.0260 近日:0.0256 节奏:0.0205 地方:0.0161 实验:0.0085 湿地:0.0081 手法:0.0077 领域:0.0077 喧嚣:0.0071 携手:0.0070  
	topic 25 >>> 冬瓜:0.0239 吃法:0.0009 日本:0.0006 绿色:0.0004 演员:0.0004 丝绸:0.0003 印记:0.0003 携手:0.0003 判断:0.0003 搜索:0.0003  
	topic 26 >>> 孩子:0.0399 医院:0.0268 道路:0.0252 记者:0.0236 民警:0.0164 男子:0.0149 市民:0.0146 学校:0.0136 校服:0.0132 学生:0.0129  
	topic 27 >>> 停电:0.0612 街道:0.0364 时间:0.0314 社区:0.0172 小区:0.0171 果子:0.0094 楼市:0.0070 天下第一:0.0069 名气:0.0067 人口:0.0050  
	topic 28 >>> 变化:0.0300 健康:0.0231 慕思:0.0181 运动:0.0126 患者:0.0112 人体:0.0079 症状:0.0064 导致:0.0056 地铁:0.0048 值得:0.0043  
	topic 29 >>> 衣物:0.0005 散发:0.0005 带来:0.0005 吃透:0.0004 琵琶:0.0004 食疗:0.0004 判断:0.0004 饺子:0.0003 影视:0.0003 上午:0.0003  
	
观察发现，近半数topic还是具有可读性的，由于每篇新闻内容相差较大，文档集过为稀疏，导致部分topic的topic-word分布非常平均，接近噪声分布，增大数据集会获得很好提升。